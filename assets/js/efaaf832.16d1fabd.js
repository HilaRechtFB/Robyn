"use strict";(self.webpackChunkmmm_for_all=self.webpackChunkmmm_for_all||[]).push([[878],{3654:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return l},contentTitle:function(){return d},metadata:function(){return h},toc:function(){return p},default:function(){return m}});var n=a(7462),o=a(3366),i=(a(7294),a(3905)),r=a(4996),s=["components"],l={id:"features",title:"Features"},d=void 0,h={unversionedId:"features",id:"features",isDocsHomePage:!1,title:"Features",description:"An in-depth discussion of both the implementation and technical underpinnings of Robyn follows -",source:"@site/docs/features2.mdx",sourceDirName:".",slug:"/features",permalink:"/Robyn/docs/features",editUrl:"https://github.com/facebookexperimental/Robyn/edit/main/website/docs/features2.mdx",tags:[],version:"current",frontMatter:{id:"features",title:"Features"},sidebar:"someSidebar",previous:{title:"Analysts guide to MMM",permalink:"/Robyn/docs/analysts-guide-to-MMM"},next:{title:"Releases",permalink:"/Robyn/docs/releases"}},p=[{value:"Inputs",id:"inputs",children:[{value:"Paid Media Variables",id:"paid-media-variables",children:[]},{value:"Organic Variables",id:"organic-variables",children:[]},{value:"Contextual Variables",id:"contextual-variables",children:[]}]},{value:"Variable Transformations",id:"variable-transformations",children:[{value:"Adstock",id:"adstock",children:[]},{value:"Diminishing returns",id:"diminishing-returns",children:[]}]},{value:"Meta Prophet",id:"meta-prophet",children:[{value:"Automated trend, seasonality and holiday effect decomposition",id:"automated-trend-seasonality-and-holiday-effect-decomposition",children:[]},{value:"Prophet decomposition plot example",id:"prophet-decomposition-plot-example",children:[]}]},{value:"Ridge Regression",id:"ridge-regression",children:[]},{value:"Nevergrad Automated Hyperparameter Selection Optimization",id:"nevergrad-automated-hyperparameter-selection-optimization",children:[]},{value:"Calibration with Experiments",id:"calibration-with-experiments",children:[]},{value:"Outputs &amp; Diagnostics",id:"outputs--diagnostics",children:[{value:"Model Solution Clustering",id:"model-solution-clustering",children:[]},{value:"Budget Allocation",id:"budget-allocation",children:[]}]},{value:"Continuous Reporting",id:"continuous-reporting",children:[]}],c={toc:p};function m(e){var t=e.components,a=(0,o.Z)(e,s);return(0,i.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"An in-depth discussion of both the implementation and technical underpinnings of Robyn follows - "),(0,i.kt)("h2",{id:"inputs"},"Inputs"),(0,i.kt)("p",null,"The function ",(0,i.kt)("inlineCode",{parentName:"p"},"robyn_inputs()")," captures all of the non-hyperparameter inputs for the model-building process. Here we break down some of the underlying concepts"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'InputCollect <- robyn_inputs(\n  dt_input = dt_simulated_weekly\n  ,dt_holidays = dt_prophet_holidays\n  ,date_var = "DATE" # date format must be "2020-01-01"\n  ,dep_var = "revenue" # there should be only one dependent variable\n  ,dep_var_type = "revenue" # "revenue" or "conversion"\n  ,context_vars = c("competitor_sales_B", "events") # e.g. competitors, discount, unemployment etc\n  ,paid_media_spends = c("tv_S","ooh_S","print_S","facebook_S", "search_S")\n  ,paid_media_vars = c("tv_S", "ooh_S,"print_S","facebook_I","search_clicks_P") \n  ,organic_vars = c("newsletter") # marketing activity without media spend\n  ,factor_vars = c("events") # specify which variables in context_vars or organic_vars are factorial\n  ,window_start = "2016-11-23"\n  ,window_end = "2018-08-22"\n  ,prophet_vars = c("trend", "season", "holiday") # "trend","season", "weekday" & "holiday"\n  ,prophet_country = "DE"# input one country. dt_prophet_holidays includes 59 countries by default\n  ,adstock = "geometric" # geometric, weibull_cdf or weibull_pdf.\n)\n')),(0,i.kt)("h3",{id:"paid-media-variables"},"Paid Media Variables"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"}," Note - As of v3.6.1 - Robyn no longer uses exposure metrics for modeling purposes, defaulting to spend metrics only for precision purposes")),(0,i.kt)("p",null,"Refer to the ",(0,i.kt)("a",{parentName:"p",href:"https://facebookexperimental.github.io/Robyn/docs/analysts-guide-to-MMM#data-collection"},"Data Collection")," section of the Analyst's Guide to MMM for a detailed discussion on best practices for selecting your paid media variables."),(0,i.kt)("p",null,"Ensure that the paid media data is complete and accurate before proceeding. We will talk more about the variable transformations paid media variables will be subject to in the Variable Transformation section."),(0,i.kt)("p",null,"For paid media variables, we currently require that users designate both ",(0,i.kt)("inlineCode",{parentName:"p"},"paid_media_vars")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"paid_media_spends"),". These lists should be of the same length, and the elements in the same positions in each list should correspond (e.g. if element 2 in paid_media_vars is your tv variable, then element 2 in paid_media_spends should be your spend that corresponds to that tv data)"),(0,i.kt)("h3",{id:"organic-variables"},"Organic Variables"),(0,i.kt)("p",null,"Robyn enables users to specify ",(0,i.kt)("inlineCode",{parentName:"p"},"organic_vars")," to model marketing activities without direct spend. Typically, this may include newsletter, push notification, social media posts, among other efforts. Moreover, organic variables are expected to have similar carryover (adstock) and saturating behavior as paid media variables. The respective transformation techniques such as geometric or Weibull transformation for adstock; or Hill transformation for saturation, are also applied to organic variables. More on these transformations in the following section - "),(0,i.kt)("hr",null),(0,i.kt)("h4",{id:"examples-of-typical-organic-variables"},"Examples of typical organic variables"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Reach / Impressions on blog posts"),(0,i.kt)("li",{parentName:"ul"},"Impressions on organic & unpaid social media"),(0,i.kt)("li",{parentName:"ul"},"SEO improvements"),(0,i.kt)("li",{parentName:"ul"},"Email campaigns"),(0,i.kt)("li",{parentName:"ul"},"Reach on UGC")),(0,i.kt)("p",null,"Below a chart showing the different types of organic variables that could be modeled"),(0,i.kt)("img",{alt:"organic media",src:(0,r.Z)("/img/organic-media.png")}),(0,i.kt)("hr",null),(0,i.kt)("h3",{id:"contextual-variables"},"Contextual Variables"),(0,i.kt)("p",null,"All contextual variables must be specified as elements in ",(0,i.kt)("inlineCode",{parentName:"p"},"context_vars"),". For a detailed discussion on potential contextual variables to include in your model, see the ",(0,i.kt)("a",{parentName:"p",href:"https://facebookexperimental.github.io/Robyn/docs/analysts-guide-to-MMM#data-collection"},"Data Collection")," section of the Analyst's guide to MMM."),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"variable-transformations"},"Variable Transformations"),(0,i.kt)("p",null,"There are two main variable transformations in the code:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Adstock"),(0,i.kt)("li",{parentName:"ol"},"Diminishing returns")),(0,i.kt)("hr",null),(0,i.kt)("h3",{id:"adstock"},"Adstock"),(0,i.kt)("p",null,"This technique is very useful for the better and more accurate representation of\nthe real carryover effect of marketing campaigns. Moreover, it helps to improve\nthe understanding of decay effects and how this can be used in campaign\nplanning. It reflects the theory that the effects of advertising can lag and\ndecay following initial exposure. In other words, not all effects of advertising\nare felt immediately\u2014memory builds and people sometimes delay action\u2014and this\nawareness diminishes as time passes. When thinking about what makes sense for adstocks, think about how they relate to each other. In other words, think about how you would expect adstock curves to look for channels in different funnel stages and whether or not they meet those hypotheses."),(0,i.kt)("p",null,"There are two adstock techniques you may choose from the code:"),(0,i.kt)("h4",{id:"geometric"},"Geometric:"),(0,i.kt)("p",null,"   Traditionally the exponential decay function is used and\ncontrolled by theta, the decay parameter. For example, an ad-stock of theta =\n0.75 means that 75% of the impressions in Period 1 were brought to Period 2.\nMathematically the traditional exponential adstock decay effect is defined\nas:"),(0,i.kt)("img",{alt:"Geometric Formula",src:(0,r.Z)("img/geometric.png")}),(0,i.kt)("p",null,"  Some rule of thumb estimates we have found from historically building weekly-level models are that  TV has tended to have adstock between 0.3 - 0.8, OOH/Print/Radio has had 0.1-0.4, and Digital has had 0.0 - 0.3. This is anecdotal advice so please use your best judgement when building your own models."),(0,i.kt)("img",{alt:"Example Geometric Adstocks",src:(0,r.Z)("img/adstockintro.png")}),(0,i.kt)("h4",{id:"weibull"},"Weibull:"),(0,i.kt)("p",null,"   Weibull survival function (Weibull distribution) provides much more\nflexibility in the shape and scale of the distribution. The formula is\ndefined as: ",(0,i.kt)("img",{alt:"Weibull Formula",src:(0,r.Z)("img/weibull.png")}),"\nWhereas a geometric adstock has one hyperparameter (theta) weibull adstocks have two hyperparameters (shape and scale or t and j in the formula). This allows Weibull adstocks to be more flexible in how they model the carryover effect, as you can see by the example plots below. This flexibility does come at a cost of added computational complexity as adding any additional hyper parameters will, but it can be worth it given the speed of Robyn's computation, especially if you suspect that a certain channel does not have a simple geometric shape."),(0,i.kt)("img",{alt:"Example Weibull Adstocks",src:(0,r.Z)("img/weibulladstocks.png")}),(0,i.kt)("p",null," ",(0,i.kt)("strong",{parentName:"p"},"Weibull CDF adstock:")," The Cumulative Distribution Function of Weibull has two parmeters\n, shape & scale, and has flexible decay rate, compared to Geometric adstock with fixed\ndecay rate. The shape parameter controls the shape of the decay curve. Recommended\nbound is c(0.0001, 2). The larger the shape, the more S-shape. The smaller, the more\nL-shape. Scale controls the inflexion point of the decay curve. We recommend very\nconservative bounce of c(0, 0.1), because scale increases the adstock half-life greatly."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Weibull PDF adstock:")," The Probability Density Function of the Weibull also has two\nparameters, shape & scale, and also has flexible decay rate as Weibull CDF. The\ndifference is that Weibull PDF offers lagged effect. When shape > 2, the curve peaks\nafter x = 0 and has NULL slope at x = 0, enabling lagged effect and sharper increase and\ndecrease of adstock, while the scale parameter indicates the limit of the relative\nposition of the peak at x axis; when 1 < shape < 2, the curve peaks after x = 0 and has\ninfinite positive slope at x = 0, enabling lagged effect and slower increase and decrease\nof adstock, while scale has the same effect as above; when shape = 1, the curve peaks at\nx = 0 and reduces to exponential decay, while scale controls the inflexion point; when\n0 < shape < 1, the curve peaks at x = 0 and has increasing decay, while scale controls\nthe inflexion point. When all possible shapes are relevant, we recommend c(0.0001, 10)\nas bounds for shape; when only strong lagged effect is of interest, we recommend\nc(2.0001, 10) as bound for shape. In all cases, we recommend conservative bound of\nc(0, 0.1) for scale. Due to the great flexibility of Weibull PDF, meaning more freedom\nin hyperparameter spaces for Nevergrad to explore, it also requires larger iterations\nto converge."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Implementation of adstocking is done in a few steps")),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"In ",(0,i.kt)("inlineCode",{parentName:"li"},"InputCollect")," set ",(0,i.kt)("inlineCode",{parentName:"li"},"adstock")," equal to the distribution you plan to use (geometric, weibull_cdf, weibull_pdf)"),(0,i.kt)("li",{parentName:"ol"},"Run ",(0,i.kt)("inlineCode",{parentName:"li"},"hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)")," to identify the hyperparameters that will need to be set up correctly in order to begin the modeling process. All paid and organic media variables will have hyper parameters that need to be set up."),(0,i.kt)("li",{parentName:"ol"},"Set ranges for each hyper parameter")),(0,i.kt)("hr",null),(0,i.kt)("h3",{id:"diminishing-returns"},"Diminishing returns"),(0,i.kt)("p",null,"The theory of diminishing returns holds that each additional unit of advertising\nincreases the response, but at a declining rate. This key marketing principle is\nreflected in marketing mix models as a variable transformation."),(0,i.kt)("img",{alt:"Diminishing returns1",src:(0,r.Z)("img/diminishingreturns1.png")}),(0,i.kt)("p",null,"The nonlinear response to a media variable on the dependent variable can be\nmodelled using a variety of functions. For example, we can use a simple\nlogarithm transformation (taking the log of the units of advertising log(x) ),\nor a power transformation (x^alpha). In the case of a power transformation, the\nmodeler tests the different variables (different levels of parameter x) for the\nhighest significance of the variable in the model and the highest significance\nof the equation overall. However, the most common approach is to use the\nflexible S-curve (Hill) transformation:"),(0,i.kt)("img",{alt:"Diminishing returns1",src:(0,r.Z)("img/diminishingreturns2.png")}),(0,i.kt)("p",null,"The variations of the parameters give modelers full flexibility on the look of\nthe S-curve, specifically the shape and the inflection points:"),(0,i.kt)("img",{alt:"Diminishing returns1",src:(0,r.Z)("img/diminishingreturns3.png")}),(0,i.kt)("p",null,"To understand these charts, on the x axis we have spend, and on the y-axis we have response. So as the spend rises, the response changes and depending on the curve we understand what the marginal response is. These curves are extremely useful in the end stages of MMM where we seek to understand how we can more optimally allocate budgets between all of our media channels."),(0,i.kt)("p",null,"Hill function for saturation: The Hill function is a two-parametric function in Robyn with\nalpha and gamma. Alpha controls the shape of the curve between exponential and s-shape.\nRecommended bound is c(0.5, 3). The larger the alpha, the more S-shape. The smaller, the\nmore C-shape. Gamma controls the inflection point. Recommended range is c(0.3, 1). The\nlarger the gamma, the later the inflection point in the response curve."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Implementation in Robyn is all done in hyper parameter setting")),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Ensure you have a alpha and gamma hyper parameter for each paid media variable with a range specified")),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"meta-prophet"},"Meta Prophet"),(0,i.kt)("h3",{id:"automated-trend-seasonality-and-holiday-effect-decomposition"},"Automated trend, seasonality and holiday effect decomposition"),(0,i.kt)("hr",null),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Prophet")," has been included in the code in order to improve the fit and forecast of\ntime-series by decomposing the effect of trend, seasonality and holiday\ncomponents in the response variable (Sales, conversions, etc.)."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Prophet")," is a Meta original procedure for ",(0,i.kt)("strong",{parentName:"p"},"forecasting time\nseries")," data, based on a model where ",(0,i.kt)("strong",{parentName:"p"},"non-linear trends")," are fit with yearly,\nweekly, and daily seasonality, plus holiday effects. It works best with time\nseries that have strong seasonal effects and several seasons of historical data.\nMore details can be found ",(0,i.kt)("a",{parentName:"p",href:"https://facebook.github.io/prophet/docs/"},"here"),"."),(0,i.kt)("hr",null),(0,i.kt)("h3",{id:"prophet-decomposition-plot-example"},"Prophet decomposition plot example"),(0,i.kt)("p",null,'Trend, season, holiday and an extra regressor ("events" in this case) decomposition by Prophet.\nWeekday is not used because the sample data is weekly (not daily).\nRobyn uses Prophet to also decompose categorical variables as an extra regressor which can simplify later programming.\nFor technical details of decomposition, please refer to Prophet\'s documentation ',(0,i.kt)("a",{parentName:"p",href:"https://facebook.github.io/prophet/docs/trend_changepoints.html"},"here"),"."),(0,i.kt)("img",{alt:"prophet 2",src:(0,r.Z)("/img/prophet_decomp.png")}),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"ridge-regression"},"Ridge Regression"),(0,i.kt)("p",null,"In order to address multicollinearity among many regressors and prevent\noverfitting we apply a regularization technique to reduce variance at the cost\nof introducing some bias. This approach tends to improve the predictive\nperformance of MMMs. The most common regularization, and the one we are using in\nthis code is Ridge regression. The mathematical notation for Ridge regression\nis:"),(0,i.kt)("img",{alt:"Ridge Regression Formula",src:(0,r.Z)("img/Ridge.png")}),(0,i.kt)("p",null,"If we go a bit deeper into the actual components we will be using within the model specification,\nbesides the lambda penalization term above, we can identify the following formula:"),(0,i.kt)("img",{alt:"Ridge Regression Formula",src:(0,r.Z)("img/model_specification.png")}),(0,i.kt)("p",null,"Ridge regression has an additional benefit of being relatively easy to interpret compared to other more complex techniques. As you can see in the formula, the hyperparameters that we set for each variable are used in this equation. As you'll see in the following section, we use automated hyper parameter optimization in order to ensure we get the best fitting ridge regression model."),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"nevergrad-automated-hyperparameter-selection-optimization"},"Nevergrad Automated Hyperparameter Selection Optimization"),(0,i.kt)("hr",null),(0,i.kt)("p",null,"MMMs are likely to ",(0,i.kt)("strong",{parentName:"p"},"contain high cardinality of parameters"),". ie. alphas and gammas for the diminishing returns (Hill) function, as well as, thetas for geometric ad stock transformation.\nIn addition, parameters dimensionality increases proportionally with the total number of marketing channels to be measured. Thus, it is extremely necessary to deal with a high dimensionality parameter space where, the greater the number of parameters, ",(0,i.kt)("strong",{parentName:"p"},"the greater the model complexity, its dimensionality and computational requirements.")),(0,i.kt)("p",null,"In order to achieve computational efficiency while optimizing overall model accuracy, we leverage ",(0,i.kt)("a",{parentName:"p",href:"https://facebookresearch.github.io/nevergrad/"},(0,i.kt)("strong",{parentName:"a"},"Meta\u2019s Nevergrad gradient-free optimization platform")),".  Nevergrad allows us to optimize the explore and exploit balance through the ",(0,i.kt)("strong",{parentName:"p"},"ask")," and ",(0,i.kt)("strong",{parentName:"p"},"tell")," commands, in order to perform a multi-objective optimization tha balances out the Normalized Root Mean Square Error (",(0,i.kt)("strong",{parentName:"p"},"NRMSE"),") and ",(0,i.kt)("strong",{parentName:"p"},"decomp.RSSD")," ratio (Relationship between spend share and channels coefficient decomposition share) providing a set of ",(0,i.kt)("strong",{parentName:"p"},"Pareto optimal model solutions")),(0,i.kt)("p",null,"Please find below an example of a common chart for the Pareto model solutions.\nEach dot in the chart represents an explored model solution, while the lower-left corner lines are Pareto-fronts 1-3 and contains the best possible model results from all iterations.\nThe two axes (NRMSE on x and DECOMP.RSSD on y) are the two objective functions to be minimized.\nAs the iteration increases, a trend down the lower-left corner of the coordinate can be clearly observed.\nThis is a proof of Nevergrad's ability to drive the model result towards an optimal direction."),(0,i.kt)("img",{alt:"pareto chart",src:(0,r.Z)("/img/pareto_front.png")}),(0,i.kt)("p",null,"The premise of an ",(0,i.kt)("strong",{parentName:"p"},"evolutionary algorithm")," is that of natural selection. In an evolutionary algorithm you may have a set of iterations where some combinations of coefficients that will be explored by the model will survive and proliferate, while unfit models will die off and not contribute to the gene pool of further generations, much like in natural selection.\nIn robyn, we recommend a minimum of 2000 iterations where each of these will provide feedback to its upcoming generation, and therefore guide the model towards the optimal coefficient values for alphas, gammas and thetas. We also recommend a minimum of 5 trials which are a set of independent initiations of the model that will each of them have the number of iterations you set under \u2018set_iter\u2019 object. E.g. 2000 iterations on set_iter x 5 trials = 10000 different iterations and possible model solutions."),(0,i.kt)("p",null,"In Robyn, we consider the model to be converged ",(0,i.kt)("strong",{parentName:"p"}," UNDER REVISION ")," when:"),(0,i.kt)("p",null,"Criteria #1:\nLast quantile's standard deviation < first 3 quantiles' mean standard deviation"),(0,i.kt)("p",null,"Criteria #2:\nLast quantile's absolute median < absolute first quantile's absolute median - 2 * first 3 quantiles' mean standard deviation"),(0,i.kt)("p",null,"The quantiles are ordered by the iterations of the model, so if we ran 1000 iterations, the first 200 iterations would make up the first quantile. These two criteria represent an effort to demonstrate that both the standard deviation and the mean for both NRMSE and DECOMP.RSSD have improved relative to where they began, and they are not as variable."),(0,i.kt)("p",null,"After the modeling is complete, you can run:"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"OutputModels$convergence$moo_distrb_plot")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"OutputModels$convergence$moo_cloud_plot")),(0,i.kt)("p",null,"To investigate the convergence of your multi-objective optimization. See below for examples of these graphs."),(0,i.kt)("img",{alt:"moo distrb plot",src:(0,r.Z)("img/moo_distrb_plot.png")}),(0,i.kt)("img",{alt:"moo cloud plot",src:(0,r.Z)("img/moo_cloud_plot.png")}),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"calibration-with-experiments"},"Calibration with Experiments"),(0,i.kt)("p",null,"By applying results from randomized controlled-experiments, you may improve the\naccuracy of your marketing mix models dramatically. It is recommended to run\nthese on a recurrent basis to keep the model calibrated permanently. In general,\nwe want to compare the experiment result with the MMM estimation of a marketing\nchannel. Conceptually, this method is like a Bayesian method, in which we use\nexperiment results as a prior to shrink the coefficients of media variables. A\ngood example of these types of experiments is Facebook\u2019s conversion lift tool\nwhich can help guide the model towards a specific range of incremental values."),(0,i.kt)("img",{alt:"Calibration chart",src:(0,r.Z)("/img/calibration1.png")}),(0,i.kt)("p",null,"The figure illustrates the calibration process for one MMM candidate model.\n",(0,i.kt)("a",{parentName:"p",href:"https://facebookresearch.github.io/nevergrad/"},(0,i.kt)("strong",{parentName:"a"},"Facebook\u2019s Nevergrad gradient-free optimization platform"))," allows us to include the ",(0,i.kt)("strong",{parentName:"p"},"MAPE(cal,fb)")," as a third optimization score besides Normalized Root Mean Square Error (",(0,i.kt)("strong",{parentName:"p"},"NRMSE"),") and ",(0,i.kt)("strong",{parentName:"p"},"decomp.RSSD")," ratio (Please refer to the automated hyperparameter selection and optimization for further details) providing a set of ",(0,i.kt)("strong",{parentName:"p"},"Pareto optimal model solutions")," that minimize and converge to a set of Pareto optimal model candidates. This calibration method can be applied to other media channels which run experiments, the more channels that are calibrated, the more accurate the MMM model."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Implementation of calibration is as follows")),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Create a data file corresponding to your experimental results - here is an example")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'dt_calibration <- data.frame(\n  channel = c("facebook_S",  "tv_S", "facebook_S")\n      #channel name must in paid_media_vars\n  , liftStartDate = as.Date(c("2018-05-01", "2017-11-27", "2018-07-01"))\n     #liftStartDate must be within input data range\n  , liftEndDate = as.Date(c("2018-06-10", "2017-12-03", "2018-07-20"))\n     #liftEndDate must be within input data range\n  , liftAbs = c(400000, 300000, 200000) # Provided value must be\n     #tested on same campaign level in model and same metric as dep_var_type\n)\n')),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Set ",(0,i.kt)("inlineCode",{parentName:"p"},"calibration_input <- dt_calibration")," (or whatever you name your calibration data frame) within InputCollect")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Increase your iteration count by at least 1000 to account for needing to optimize 3 objective functions (NRMSE, Decomp.RSSD, MAPE.lift) rather than 2 (NRMSE, Decomp.RSSD)")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Run model"))),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"outputs--diagnostics"},"Outputs & Diagnostics"),(0,i.kt)("p",null,"The MMM code will automatically generate a set of plots under the folder you specify on the \u2018model_output_collect\u2019 object. Each of these plots represents one of the optimal model solutions as a result of the multi-objective optimization Pareto optimal process mentioned in the ",(0,i.kt)("strong",{parentName:"p"},"\u2018Automated hyperparameter selection and optimization\u2019")," section. Please find below an example of the model output:"),(0,i.kt)("img",{alt:"ModelResults1 chart",src:(0,r.Z)("/img/modelresults2.png")}),(0,i.kt)("p",null,"As you may observe we have 6 different charts above:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Response decomposition waterfall by predictor:")," This chart reflects the percentage of each of the variables effect (Baseline and Media variables + intercept) on the response variable. E.g. If newsletter effect says it's 9.6% that means that 9.6% of the total sales can be attributed to newsletter activations."),(0,i.kt)("p",{parentName:"li"},"Tip: Intercept, and Prophet vars like trend can constitute a large portion of your decomp. This makes sense if your brand is established, as it is basically saying that you would still have a large baseline of sales without paid media spend.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Share of spend vs. share of effect:")," This plot reflects the comparison of the total effect each channel had by means of the decomposition of the coefficients into the response variable divided by the total effect. As well as, the total spend (cost or investment) each channel had and its relative share over total marketing spend. We also plot the return on investment (ROI) each channel had which can give you an idea over the most profitable channels."),(0,i.kt)("p",{parentName:"li"},"Tip: Decomp.RSSD is one of the objective functions we minimize which corresponds to the distance between share of spend and share of effect. So in essence we do not want to see extreme distances between share of spend and share of effect because it does not make realistic business sense to optimize to. Consider this when comparing model solutions.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Average adstock decay rate:")," This chart represents, on average, what is the percentage decay rate each channel had. The higher the decay rate, the longer the effect in time for that specific channel media exposure. This is a simple calculation for Geometric adstocks, with average adstock decay rate = theta. For Weibull adstocks this calculation is a bit more complicated."),(0,i.kt)("p",{parentName:"li"},"Tip: It can be useful to look at the adstock rates in comparison to each other. For example, how do your upper funnel channel adstocks compare to your lower funnel channel adstocks. There are some logical conclusions to make about upper funnel having longer adstocks than lower funnel, so keep an eye out for cases where it doesn't seem to be the case. This isn't a hard and fast rule, but can be a way to differentiate between solutions.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Actual vs. predicted response:")," This plot shows the actual data for the response variable E.g sales, and how the modeled predicted data for that response variable is capturing the real curve. We aim for models that can capture most of the variance from the actual data and therefore the R-squared is closer to 1 while NRMSE is low."),(0,i.kt)("p",{parentName:"li"},"Tip: Look for specific periods where the model is predicting worse/better. For example, if one notices the model is predicting noticeably worse around certain periods related to promotions periods, it may be a good way to identify a contextual variable that should be included in the model")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Response curves and mean spend by channel:")," These are the diminishing returns response curves from hill function. They represent how saturated a channel is and therefore, may suggest potential budget reallocation strategies. The faster the curves reach to an inflection point and to a horizontal/flat slope, the quicker they will saturate with each extra ($) spent."),(0,i.kt)("p",{parentName:"li"},"Tip: You can use the ",(0,i.kt)("inlineCode",{parentName:"p"},"robyn_response")," function in a loop to recreate these response curves. For example, creating a loop around an execution like this:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},'Spend1 <- 60000\nResponse1 <- robyn_response(\nrobyn_object = robyn_object\n#, select_build = 1 # 2 means the second refresh model. 0 means the initial model\n, media_metric = "search_S"\n, metric_value = Spend1)\nResponse1$response/Spend1 # ROI for search 80k\n'))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Fitted vs. residual:")," This chart shows the relationship between fitted and residual values. A residual value is a measure of how much a regression line vertically misses a data point.  A residual plot is typically used to find problems with a regression. Some data sets are not good candidates for regression, such as points at widely varying distances from the line. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a nonlinear model is more appropriate."))),(0,i.kt)("h3",{id:"model-solution-clustering"},"Model Solution Clustering"),(0,i.kt)("p",null,"Rather than exploring all Pareto-optimal solutions (of which there could be dozens), we use clustering to find similar solutions and encourage users to explore those solutions as the most distinct, best solutions. The process for the clustering is as follows:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Using the pool of models on Pareto fronts obtained, we run k-means clustering using (normalized) ROI data on each (paid) media variable"),(0,i.kt)("li",{parentName:"ol"},"When ",(0,i.kt)("inlineCode",{parentName:"li"},'k = "auto"'),' (which is the default), we calculate the WSS (Within Group Sum of Squares) on k-means clustering using k = 1 to k = 20 to find the "best automatic k". We pick the one k whose WSS reduction doesn\'t change more than 5% from the previous k. You could also override this by setting k manually (on ',(0,i.kt)("inlineCode",{parentName:"li"},"robyn_run(..., cluster = TRUE)"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"robyn_outputs(..., clusters = TRUE)"),", or ",(0,i.kt)("inlineCode",{parentName:"li"},"robyn_clusters()"),")."),(0,i.kt)("li",{parentName:"ol"},'After we have run k-means on all Pareto front models, using the defined k, we pick those "best models" that have the lowest normalized combined errors (NRMSE, DECOM.RSSD, and MAPE if calibrated was used).')),(0,i.kt)("p",null,"When you run robyn_clusters() you'll get a list of results: the data used to calculate the clusters with each model's cluster-ID, and some visualizations on WSS-k selection, ROI per media on winner models, and correlations of ROI by cluster to understand each cluster's content models better. See below for example output on clustering selection."),(0,i.kt)("img",{alt:"Pareto clusters WSS",src:(0,r.Z)("img/pareto_clusters_wss.png")}),(0,i.kt)("img",{alt:"Pareto cluster details",src:(0,r.Z)("img/pareto_clusters_detail.png")}),(0,i.kt)("hr",null),(0,i.kt)("h3",{id:"budget-allocation"},"Budget Allocation"),(0,i.kt)("p",null,"Once you have analyzed the optimal model results plots and have chosen your model, you may introduce the model unique ID from the results in the previous section. E.g setting select_model = \"1_92_12\" could be an example of a selected model from the list of best models in 'OutputCollect$allSolutions' results object. Once you run the budget allocator, results will be plotted and saved under the same folder where the model plots had been saved. The result would look like the following:"),(0,i.kt)("img",{alt:"budget allocator chart",src:(0,r.Z)("/img/optimizer_new.png")}),(0,i.kt)("p",null,"You may encounter three charts as in the example above:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Initial vs. optimized budget allocation:")," This channel shows the original spend share vs. the new optimized recommended one. If optimized share is greater than original, this would mean you will need to proportionally increase budgets for that channel according to the difference between both shares. And you would reduce budgets in the case spend share would be greater than optimized share."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Initial vs. optimized mean response:")," Similar to the chart above, we have initial and optimized shares, but this time over the total expected response E.g. Sales. The optimized response is the total increase in sales that we are expecting you to have if you switch budgets following the chart we explained above, increasing those with better share for optimized spend and decreasing spend for those with lower optimized spend than the initial."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Response curve and mean spend by channel:")," These again are the diminishing returns response curves from hill function. They represent how saturated a channel is and therefore, may suggest potential budget reallocation strategies. The faster the curves reach to an inflection point and to a horizontal/flat slope, the quicker they will saturate with each extra ($) spent. The initial mean spend is represented by a circle and the optimized one by the triangle.")),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"continuous-reporting"},"Continuous Reporting"),(0,i.kt)("hr",null),(0,i.kt)("p",null,"Another strong capability requirement linked to the model window we identified, is the ability to refresh the model when new data would arrive. In other words, to be able to continuously report in a monthly, weekly or even daily frequency, based on a previously selected model but applied onto new fresh data. Consequently, enabling MMM to be a continuous reporting tool for actionable and timely decision-making that could feed your reporting or BI tools within the defined cadence."),(0,i.kt)("p",null,"The new ",(0,i.kt)("inlineCode",{parentName:"p"},"robyn_refresh()")," function is able to continuously build and add new model periods, at any given cadence (weeks, months, etc.), based on previously selected models saved in the Robyn.RData object specified in the robyn_object."),(0,i.kt)("p",null,"For example, when updating the initial build with 4 weeks of new data, robyn_refresh() consumes the selected model of the initial build. What Robyn does is to set lower and upper bounds of hyperparameters for the new build which are consistent with the selected hyperparameters of the previous build. Therefore, stabilizing the effect of contextual and organic variables across old and new builds as well as, regulating the new share of effect of media variables towards the new added period spend level. Finally, returning aggregated results containing all previous builds for reporting purposes and their corresponding plots."),(0,i.kt)("hr",null),(0,i.kt)("p",null,"The example below shows the model refreshing mechanism for 5 different periods of time based in an initial window covering most of 2017 and 2018:"),(0,i.kt)("img",{alt:"pareto chart 2",src:(0,r.Z)("/img/refresh-window.png")}),(0,i.kt)("hr",null),(0,i.kt)("p",null,"You will also obtain a set of results for each refresh period that describes the assigned ROI and effects from each of the variables within the model. The baseline variable is the sum of all prophet variables (trend, seasonality, weekday and holiday) plus the intercept. The charts are based on simulated and do not have real-life implication:"),(0,i.kt)("img",{alt:"pareto chart 2",src:(0,r.Z)("/img/refresh-reporting.png")}),(0,i.kt)("hr",null),(0,i.kt)("h4",{id:"robyn_refresh-description"},(0,i.kt)("inlineCode",{parentName:"h4"},"robyn_refresh()")," description"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"robyn_refresh()")," function builds updated models based on the previously built models saved in the Robyn.RData object specified in robyn_object.\nFor example, when updating the initial build with 4 weeks of new data, ",(0,i.kt)("inlineCode",{parentName:"p"},"robyn_refresh()")," consumes the selected model of the initial build."),(0,i.kt)("p",null,"What Robyn does, is to set lower and upper bounds of hyperparameters for the new build around the selected hyperparameters of the previous build,\nstabilizes the effect of baseline variables across old and new builds and regulates the new effect share of media variables towards the latest spend level.\nIt returns aggregated results with all previous builds for reporting purposes and produces reporting plots."),(0,i.kt)("hr",null))}m.isMDXComponent=!0}}]);